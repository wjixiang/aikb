/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ npm install @boundaryml/baml
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code

const fileMap = {
  'chat.baml':
    '// Simple chat function for the library chatbot\nfunction ChatResponse(message: string) -> string {\n  client GLM4Plus // Set OPENAI_API_KEY to use this client.\n  prompt #"\n    {{_.role("system")}}\n    You are a helpful library assistant. Respond to the user\'s message in a friendly and helpful way.\n    \n    {{_.role("user")}}\n    {{ message }}\n    \n    {{ ctx.output_format }}\n  "#\n}',
  'clients.baml':
    'client<llm> GLM4Plus {\n  provider openai-generic\n  options {\n    model "glm-4-plus"\n    api_key env.GLM_API_KEY\n    base_url "https://open.bigmodel.cn/api/paas/v4"\n    retry_policy GLMRetry\n  }\n}\nclient<llm> GLM45Flash {\n  provider openai-generic\n  options {\n    model "glm-4.5-flash"\n    api_key env.GLM_API_KEY\n    base_url "https://open.bigmodel.cn/api/paas/v4"\n    retry_policy GLMRetry\n  }\n}\n\nclient<llm> GLM45Air {\n  provider openai-generic\n  options {\n    model "glm-4-air-250414"\n    api_key env.GLM_API_KEY\n    base_url "https://open.bigmodel.cn/api/paas/v4"\n    retry_policy GLMRetry\n  }\n}\n\nclient<llm> GLM46 {\n  provider openai-generic\n  options {\n    model "glm-4.6"\n    api_key env.GLM_API_KEY\n    base_url "https://open.bigmodel.cn/api/paas/v4"\n    retry_policy GLMRetry\n    thinking {\n      type "disabled"\n    }\n  }\n}\n\n\n\n\nclient<llm> QiniuDeepseekV3 {\n  provider openai-generic\n  options {\n    model "deepseek-v3"\n    api_key env.QINIU_DEEPSEEK_FREE_API_KEY\n    base_url "https://api.qnaigc.com/v1"\n  }\n}\n\nclient<llm> GeminiFlash {\n  provider openai-generic\n  options {\n    model "gemini-2.5-flash-preview-05-20"\n    api_key env.SHENMA_API_KEY\n    base_url "https://api.whatai.cc/v1"\n  }\n}\n\nclient<llm> GeminiFlashOpenrouter {\n  provider openai-generic\n  options {\n    model "google/gemini-2.5-pro"\n    api_key env.OPENROUTER_API_KEY\n    base_url "https://openrouter.ai/api/v1"\n  }\n}\n\nclient<llm> Kimi {\n  provider openai-generic\n  options {\n    model "kimi-k2-0905-preview"\n    api_key env.KIMI_API_KEY\n    base_url "https://api.moonshot.cn/v1"\n  }\n}\n\nclient<llm> KimiK2Turbo {\n  provider openai-generic\n  options {\n    model "kimi-k2-turbo-preview"\n    api_key env.KIMI_API_KEY\n    base_url "https://api.moonshot.cn/v1"\n  }\n}\n\n\nretry_policy GLMRetry {\n  max_retries 6\n    strategy {\n    type constant_delay\n    delay_ms 10000\n  }\n}\n',
  'generators.baml':
    '// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"\n    output_type "typescript"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir "../"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version "0.214.0"\n\n    // Valid values: "sync", "async"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n',
  'resume.baml':
    '// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like "client CustomHaiku"\n  client GLM4Plus // Set OPENAI_API_KEY to use this client.\n  prompt #"\n    {{_.role("user")}}\n\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  "#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    "#\n  }\n}\n',
};
export const getBamlFiles = () => {
  return fileMap;
};
